{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6501f3ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"font-width:bold; font-size: 3rem; color:#333;\">Training Pipeline</span>\n",
    "\n",
    "## üóíÔ∏è This notebook is divided into the following sections:\n",
    "\n",
    "1. Create Feature Views\n",
    "2. Train model\n",
    "3. Validate model\n",
    "4. Save model to model registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f687e9",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928e9811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-18 14:22:32,794 INFO: generated new fontManager\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdbc563",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> üì° Connecting to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aa7a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://snurran.hops.works/p/5240\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "fs = project.get_feature_store() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed659e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature groups\n",
    "air_quality_fg = fs.get_feature_group(\n",
    "    name='air_quality',\n",
    "    version=1,\n",
    ")\n",
    "weather_fg = fs.get_feature_group(\n",
    "    name='weather',\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da73cd",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## <span style=\"color:#ff5f27;\"> üñç Feature View Creation and Retrieving </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b28669b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for training data.\n",
    "selected_features = air_quality_fg.select(['city', 'pm25']).join(\n",
    "    weather_fg.select_all(),  on=['city', 'date'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1968c34d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (0.92s) \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>pm25</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2017-10-23 00:00:00+00:00</td>\n",
       "      <td>5.299833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.777550</td>\n",
       "      <td>81.049316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2017-11-13 00:00:00+00:00</td>\n",
       "      <td>-1.093917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.599998</td>\n",
       "      <td>293.791504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2017-11-14 00:00:00+00:00</td>\n",
       "      <td>-0.914750</td>\n",
       "      <td>0.2</td>\n",
       "      <td>13.698934</td>\n",
       "      <td>248.167618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2017-12-02 00:00:00+00:00</td>\n",
       "      <td>-0.062667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.855560</td>\n",
       "      <td>238.739838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2017-12-08 00:00:00+00:00</td>\n",
       "      <td>4.033167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.675304</td>\n",
       "      <td>204.792206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        city  pm25                      date  temperature_2m_mean  \\\n",
       "0  stockholm  11.0 2017-10-23 00:00:00+00:00             5.299833   \n",
       "1  stockholm  10.0 2017-11-13 00:00:00+00:00            -1.093917   \n",
       "2  stockholm  12.0 2017-11-14 00:00:00+00:00            -0.914750   \n",
       "3  stockholm   9.0 2017-12-02 00:00:00+00:00            -0.062667   \n",
       "4  stockholm  14.0 2017-12-08 00:00:00+00:00             4.033167   \n",
       "\n",
       "   precipitation_sum  wind_speed_10m_max  wind_direction_10m_dominant  \n",
       "0                0.0           14.777550                    81.049316  \n",
       "1                0.0           12.599998                   293.791504  \n",
       "2                0.2           13.698934                   248.167618  \n",
       "3                0.0           19.855560                   238.739838  \n",
       "4                0.0           23.675304                   204.792206  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment this if you would like to view your selected features\n",
    "selected_features.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecf7e6a",
   "metadata": {},
   "source": [
    "`Feature Views` stands between **Feature Groups** and **Training Dataset**. –°ombining **Feature Groups** we can create **Feature Views** which store a metadata of our data. Having **Feature Views** we can create **Training Dataset**.\n",
    "\n",
    "The Feature Views allows schema in form of a query with filters, define a model target feature/label and additional transformation functions.\n",
    "\n",
    "In order to create Feature View we can use `FeatureStore.create_feature_view()` method.\n",
    "\n",
    "You can specify next parameters:\n",
    "\n",
    "- `name` - name of a feature group.\n",
    "\n",
    "- `version` - version of a feature group.\n",
    "\n",
    "- `labels`- our target variable.\n",
    "\n",
    "- `transformation_functions` - functions to transform our features.\n",
    "\n",
    "- `query` - query object with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ff288c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature view created successfully, explore it at \n",
      "https://snurran.hops.works/p/5240/fs/5188/fv/air_quality_fv/version/1\n"
     ]
    }
   ],
   "source": [
    "# Get or create the 'air_quality_fv' feature view\n",
    "feature_view = fs.get_or_create_feature_view(\n",
    "    name='air_quality_fv',\n",
    "    version=1,\n",
    "    labels=['pm25'],\n",
    "    query=selected_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9619db6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at \n",
      "https://snurran.hops.works/p/5240/jobs/named/air_quality_fv_1_create_fv_td_18022024161536/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `2`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, <hsfs.core.job.Job at 0x7fbf10599b70>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_version, td_job = feature_view.create_train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db973731",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4200422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using ArrowFlight (2.00s) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data type could not be inferred for column 'pm25'. Defaulting to 'String'\n",
      "Data type could not be inferred for column 'temperature_2m_mean'. Defaulting to 'String'\n",
      "Data type could not be inferred for column 'precipitation_sum'. Defaulting to 'String'\n",
      "Data type could not be inferred for column 'wind_speed_10m_max'. Defaulting to 'String'\n",
      "Data type could not be inferred for column 'wind_direction_10m_dominant'. Defaulting to 'String'\n",
      "Data type could not be inferred for column 'date'. Defaulting to 'String'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Timestamp is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0bfd7dc42517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_view\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/usage.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Disable usage AFTER import hsfs, return function itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/feature_view.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(self, test_size, train_start, train_end, test_start, test_end, description, extra_filter, statistics_config, read_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m   2203\u001b[0m             \u001b[0mextra_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextra_filter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         )\n\u001b[0;32m-> 2205\u001b[0;31m         td, df = self._feature_view_engine.get_training_data(\n\u001b[0m\u001b[1;32m   2206\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m             \u001b[0mread_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mget_training_data\u001b[0;34m(self, feature_view_obj, read_options, splits, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0mtd_updated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             self.compute_training_dataset_statistics(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mfeature_view_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_updated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/feature_view_engine.py\u001b[0m in \u001b[0;36mcompute_training_dataset_statistics\u001b[0;34m(self, feature_view_obj, training_dataset_obj, td_df)\u001b[0m\n\u001b[1;32m    625\u001b[0m                         \u001b[0;34m\"'split': dataframe\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     )\n\u001b[0;32m--> 627\u001b[0;31m                 return self._statistics_engine.compute_and_save_split_statistics(\n\u001b[0m\u001b[1;32m    628\u001b[0m                     \u001b[0mtraining_dataset_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                     \u001b[0mfeature_dataframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtd_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\u001b[0m in \u001b[0;36mcompute_and_save_split_statistics\u001b[0;34m(self, td_metadata_instance, feature_view_obj, feature_dataframes)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd_metadata_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0msplit_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             stats_str = self.profile_statistics_with_config(\n\u001b[0m\u001b[1;32m    200\u001b[0m                 (\n\u001b[1;32m    201\u001b[0m                     \u001b[0mfeature_dataframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\u001b[0m in \u001b[0;36mprofile_statistics_with_config\u001b[0;34m(feature_dataframe, statistics_config)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mSerialized\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0mstatistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[0;32m--> 147\u001b[0;31m         return StatisticsEngine.profile_statistics(\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0mfeature_dataframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mstatistics_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/core/statistics_engine.py\u001b[0m in \u001b[0;36mprofile_statistics\u001b[0;34m(feature_dataframe, columns, correlations, histograms, exact_uniqueness)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mcol_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"column\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcol_stats\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         return engine.get_instance().profile(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mfeature_dataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrelations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexact_uniqueness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/site-packages/hsfs/engine/python.py\u001b[0m in \u001b[0;36mprofile\u001b[0;34m(self, df, relevant_columns, correlations, histograms, exact_uniqueness)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mfinal_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         return json.dumps(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_stats\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         )\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/srv/hops/anaconda/envs/theenv/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Timestamp is not JSON serializable"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = feature_view.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d32cd",
   "metadata": {},
   "source": [
    "For now `Feature View` is saved in Hopsworks and you can retrieve it using `FeatureStore.get_feature_view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b336b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "td_version=2\n",
    "X_train, X_test, y_train, y_test = feature_view.get_train_test_split(td_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ef3c219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>date</th>\n",
       "      <th>temperature_2m_mean</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_direction_10m_dominant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2020-07-23</td>\n",
       "      <td>14.412334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.659916</td>\n",
       "      <td>285.032990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2022-10-17</td>\n",
       "      <td>10.293582</td>\n",
       "      <td>5.8</td>\n",
       "      <td>25.420181</td>\n",
       "      <td>261.859436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2022-04-26</td>\n",
       "      <td>6.668583</td>\n",
       "      <td>0.1</td>\n",
       "      <td>15.990646</td>\n",
       "      <td>343.442841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>-0.508500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.928989</td>\n",
       "      <td>286.460083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2019-05-29</td>\n",
       "      <td>10.593583</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.052162</td>\n",
       "      <td>263.531799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>7.143584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.795301</td>\n",
       "      <td>239.564911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2018-08-23</td>\n",
       "      <td>19.756083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.073025</td>\n",
       "      <td>210.017105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2023-01-19</td>\n",
       "      <td>-1.064750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.200571</td>\n",
       "      <td>218.105850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2021-10-08</td>\n",
       "      <td>12.126919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.904173</td>\n",
       "      <td>222.140091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>stockholm</td>\n",
       "      <td>2019-06-14</td>\n",
       "      <td>17.512335</td>\n",
       "      <td>0.2</td>\n",
       "      <td>19.469975</td>\n",
       "      <td>279.241180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1800 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           city       date  temperature_2m_mean  precipitation_sum  \\\n",
       "0     stockholm 2020-07-23            14.412334                0.0   \n",
       "1     stockholm 2022-10-17            10.293582                5.8   \n",
       "2     stockholm 2022-04-26             6.668583                0.1   \n",
       "3     stockholm 2018-12-27            -0.508500                0.0   \n",
       "4     stockholm 2019-05-29            10.593583                0.0   \n",
       "...         ...        ...                  ...                ...   \n",
       "1795  stockholm 2021-04-04             7.143584                0.0   \n",
       "1796  stockholm 2018-08-23            19.756083                0.0   \n",
       "1797  stockholm 2023-01-19            -1.064750                0.0   \n",
       "1798  stockholm 2021-10-08            12.126919                0.0   \n",
       "1799  stockholm 2019-06-14            17.512335                0.2   \n",
       "\n",
       "      wind_speed_10m_max  wind_direction_10m_dominant  \n",
       "0              21.659916                   285.032990  \n",
       "1              25.420181                   261.859436  \n",
       "2              15.990646                   343.442841  \n",
       "3              11.928989                   286.460083  \n",
       "4              26.052162                   263.531799  \n",
       "...                  ...                          ...  \n",
       "1795           29.795301                   239.564911  \n",
       "1796           20.073025                   210.017105  \n",
       "1797           11.200571                   218.105850  \n",
       "1798           14.904173                   222.140091  \n",
       "1799           19.469975                   279.241180  \n",
       "\n",
       "[1800 rows x 6 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10825bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1800 entries, 0 to 1799\n",
      "Data columns (total 6 columns):\n",
      " #   Column                       Non-Null Count  Dtype         \n",
      "---  ------                       --------------  -----         \n",
      " 0   city                         1800 non-null   object        \n",
      " 1   date                         1800 non-null   datetime64[ns]\n",
      " 2   temperature_2m_mean          1800 non-null   float32       \n",
      " 3   precipitation_sum            1800 non-null   float32       \n",
      " 4   wind_speed_10m_max           1800 non-null   float32       \n",
      " 5   wind_direction_10m_dominant  1800 non-null   float32       \n",
      "dtypes: datetime64[ns](1), float32(4), object(1)\n",
      "memory usage: 56.4+ KB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88fb79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325f423d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\">üß¨ Modeling</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfa4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a LabelEncoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fitting the encoder to the data in the 'city_name' column\n",
    "label_encoder.fit(X[['city_name']])\n",
    "\n",
    "# Transforming the 'city_name' column data using the fitted encoder\n",
    "encoded = label_encoder.transform(X[['city_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c3c185",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the output of the label encoding to a dense array and concatenate with the original data\n",
    "X = pd.concat([X, pd.DataFrame(encoded)], axis=1)\n",
    "\n",
    "# Drop columns 'date', 'city_name', 'unix_time' from the DataFrame 'X'\n",
    "X = X.drop(columns=['date', 'city_name', 'unix_time'])\n",
    "\n",
    "# Rename the newly added column with label-encoded city names to 'city_name_encoded'\n",
    "X = X.rename(columns={0: \"city_name_encoded\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932373aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extracting the target variable 'pm2_5' from the DataFrame 'X' and assigning it to the variable 'y'\n",
    "y = X.pop('pm2_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855d25ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets using the train_test_split function\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92321f1b",
   "metadata": {},
   "source": [
    "### <span style='color:#ff5f27'> ‚öñÔ∏è Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015dc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing the current time as the start time of the cell execution\n",
    "start_of_cell = time.time()\n",
    "\n",
    "# Creating an instance of the XGBoost Regressor\n",
    "xgb_regressor = XGBRegressor()\n",
    "\n",
    "# Fitting the XGBoost Regressor to the training data\n",
    "xgb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predicting target values on the test set\n",
    "y_pred = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Calculating Mean Squared Error (MSE) using sklearn\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "# Calculating Root Mean Squared Error (RMSE) using sklearn\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "# Calculating R squared using sklearn\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R squared:\", r2)\n",
    "\n",
    "# Storing the current time as the end time of the cell execution\n",
    "end_of_cell = time.time()\n",
    "\n",
    "# Printing information about the execution, including the time taken\n",
    "print(f\"Took {round(end_of_cell - start_of_cell, 2)} sec.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7950d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a DataFrame 'df_' to store true and predicted values for evaluation\n",
    "df_ = pd.DataFrame({\n",
    "    \"y_true\": y_test,\n",
    "    \"y_pred\": y_pred,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa25dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating a residual plot using Seaborn\n",
    "residplot = sns.residplot(data=df_, x=\"y_true\", y=\"y_pred\", color='orange')\n",
    "\n",
    "# Adding title, xlabel, and ylabel to the residual plot\n",
    "plt.title('Model Residuals')\n",
    "plt.xlabel('Observation #')\n",
    "plt.ylabel('Error')\n",
    "\n",
    "# Displaying the residual plot\n",
    "plt.show()\n",
    "\n",
    "# Getting the figure from the residual plot and displaying it separately\n",
    "fig = residplot.get_figure()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f705dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting feature importances using the plot_importance function from XGBoost\n",
    "# 'xgb_regressor' is the trained XGBoost Regressor\n",
    "# Setting 'max_num_features' to 25 to display the top 25 most important features\n",
    "plot_importance(xgb_regressor, max_num_features=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad44c3d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c3640",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style='color:#ff5f27'>üóÑ Model Registry</span>\n",
    "\n",
    "One of the features in Hopsworks is the model registry. This is where you can store different versions of models and compare their performance. Models from the registry can then be served as API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a9aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve the model registry\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b96d5f",
   "metadata": {},
   "source": [
    "### <span style=\"color:#ff5f27;\">‚öôÔ∏è Model Schema</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ea821",
   "metadata": {},
   "source": [
    "The model needs to be set up with a [Model Schema](https://docs.hopsworks.ai/machine-learning-api/latest/generated/model_schema/), which describes the inputs and outputs for a model.\n",
    "\n",
    "A Model Schema can be automatically generated from training examples, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b56bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hsml.schema import Schema\n",
    "from hsml.model_schema import ModelSchema\n",
    "\n",
    "# Creating input and output schemas using the 'Schema' class for features (X) and target variable (y)\n",
    "input_schema = Schema(X)\n",
    "output_schema = Schema(y)\n",
    "\n",
    "# Creating a model schema using 'ModelSchema' with the input and output schemas\n",
    "model_schema = ModelSchema(input_schema=input_schema, output_schema=output_schema)\n",
    "\n",
    "# Converting the model schema to a dictionary representation\n",
    "schema_dict = model_schema.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badd2e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a directory for the model artifacts if it doesn't exist\n",
    "model_dir = \"air_quality_model\"\n",
    "if os.path.isdir(model_dir) == False:\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "# Saving the label encoder and XGBoost regressor as joblib files in the model directory\n",
    "joblib.dump(label_encoder, model_dir + '/label_encoder.pkl')\n",
    "joblib.dump(xgb_regressor, model_dir + '/xgboost_regressor.pkl')\n",
    "\n",
    "# Saving the residual plot figure as an image in the model directory\n",
    "fig.savefig(model_dir + \"/residplot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6b29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Python model in the model registry named 'air_quality_xgboost_model'\n",
    "aq_model = mr.python.create_model(\n",
    "    name=\"air_quality_xgboost_model\", \n",
    "    metrics={\n",
    "        \"RMSE\": rmse,\n",
    "        \"MSE\": mse,\n",
    "        \"R squared\": r2,\n",
    "    },\n",
    "    model_schema=model_schema,\n",
    "    input_example=X_test.sample().values, \n",
    "    description=\"Air Quality (PM2.5) predictor\",\n",
    ")\n",
    "\n",
    "# Saving the model artifacts to the 'air_quality_model' directory in the model registry\n",
    "aq_model.save(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57def5d",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:#ff5f27;\">‚è≠Ô∏è **Next:** Part 04: Batch Inference</span>\n",
    "\n",
    "In the following notebook you will use your model for Batch Inference.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "63265f9757e7c73c149a91832e3b2b12ced37a5390b9151ad08a04f276cd5846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
