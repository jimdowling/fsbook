{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3764216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from hops import hdfs\n",
    "import hopsworks\n",
    "\n",
    "proj = hopsworks.login()\n",
    "fs = proj.get_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a4e971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 99441 entries, 0 to 99440\n",
      "Data columns (total 8 columns):\n",
      " #   Column                         Non-Null Count  Dtype         \n",
      "---  ------                         --------------  -----         \n",
      " 0   order_id                       99441 non-null  object        \n",
      " 1   customer_id                    99441 non-null  object        \n",
      " 2   order_status                   99441 non-null  object        \n",
      " 3   order_purchase_timestamp       99441 non-null  datetime64[ns]\n",
      " 4   order_approved_at              99441 non-null  datetime64[ns]\n",
      " 5   order_delivered_carrier_date   99441 non-null  datetime64[ns]\n",
      " 6   order_delivered_customer_date  99441 non-null  datetime64[ns]\n",
      " 7   order_estimated_delivery_date  99441 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](5), object(3)\n",
      "memory usage: 6.8+ MB\n"
     ]
    }
   ],
   "source": [
    "orders_df = pd.read_csv(hdfs.project_path() + \"Jupyter/fs-notebooks/ecommerce/order.csv\", index_col=0, infer_datetime_format=True, \n",
    "                        parse_dates=['order_purchase_timestamp', 'order_approved_at', \n",
    "                                     'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date'])\n",
    "\n",
    "orders_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bb78cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['delivered' 'invoiced' 'shipped' 'processing' 'unavailable' 'canceled'\n",
      " 'created' 'approved']\n"
     ]
    }
   ],
   "source": [
    " print(orders_df['order_status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3cb8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring ingestion job...\n",
      "Uploading Pandas dataframe...\n",
      "Launching ingestion job...\n",
      "Ingestion Job started successfully, you can follow the progress at https://hopsworks.glassfish.service.consul:8182/hopsworks/#!/project/119/jobs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hsfs.core.job.Job at 0x7fdabdf26280>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_fg = fs.create_feature_group(name=\"orders\",\n",
    "                                        version=1,\n",
    "                                        primary_key=['order_id'],\n",
    "                                        online_enabled=True,\n",
    "                                        description=\"Orders with status\")\n",
    "orders_fg.save(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30a8c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579e2a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-06 10:25:19,498 INFO: USE `demo_fs_jim00000_featurestore`\n",
      "2022-03-06 10:25:20,285 INFO: SELECT `fg0`.`order_id`, `fg0`.`customer_id`, `fg0`.`order_status`, `fg0`.`order_purchase_timestamp`, `fg0`.`order_approved_at`, `fg0`.`order_delivered_carrier_date`, `fg0`.`order_delivered_customer_date`, `fg0`.`order_estimated_delivery_date`\n",
      "FROM `demo_fs_jim00000_featurestore`.`orders_1` `fg0`\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "orders_fg = fs.get_feature_group(name=\"orders\", version=1)\n",
    "orders_df = orders_fg.read()\n",
    "\n",
    "ct = ColumnTransformer([(\"order_status\", preprocessing.OneHotEncoder(), [2])], remainder = 'passthrough')\n",
    "\n",
    "ct = ct.fit(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6bc05b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ColumnTransformer in module sklearn.compose._column_transformer object:\n",
      "\n",
      "class ColumnTransformer(sklearn.base.TransformerMixin, sklearn.utils.metaestimators._BaseComposition)\n",
      " |  ColumnTransformer(transformers, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False)\n",
      " |  \n",
      " |  Applies transformers to columns of an array or pandas DataFrame.\n",
      " |  \n",
      " |  This estimator allows different columns or column subsets of the input\n",
      " |  to be transformed separately and the features generated by each transformer\n",
      " |  will be concatenated to form a single feature space.\n",
      " |  This is useful for heterogeneous or columnar data, to combine several\n",
      " |  feature extraction mechanisms or transformations into a single transformer.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <column_transformer>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.20\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  transformers : list of tuples\n",
      " |      List of (name, transformer, column(s)) tuples specifying the\n",
      " |      transformer objects to be applied to subsets of the data.\n",
      " |  \n",
      " |      name : string\n",
      " |          Like in Pipeline and FeatureUnion, this allows the transformer and\n",
      " |          its parameters to be set using ``set_params`` and searched in grid\n",
      " |          search.\n",
      " |      transformer : estimator or {'passthrough', 'drop'}\n",
      " |          Estimator must support :term:`fit` and :term:`transform`.\n",
      " |          Special-cased strings 'drop' and 'passthrough' are accepted as\n",
      " |          well, to indicate to drop the columns or to pass them through\n",
      " |          untransformed, respectively.\n",
      " |      column(s) : string or int, array-like of string or int, slice, boolean mask array or callable\n",
      " |          Indexes the data on its second axis. Integers are interpreted as\n",
      " |          positional columns, while strings can reference DataFrame columns\n",
      " |          by name.  A scalar string or int should be used where\n",
      " |          ``transformer`` expects X to be a 1d array-like (vector),\n",
      " |          otherwise a 2d array will be passed to the transformer.\n",
      " |          A callable is passed the input data `X` and can return any of the\n",
      " |          above. To select multiple columns by name or dtype, you can use\n",
      " |          :obj:`make_column_transformer`.\n",
      " |  \n",
      " |  remainder : {'drop', 'passthrough'} or estimator, default 'drop'\n",
      " |      By default, only the specified columns in `transformers` are\n",
      " |      transformed and combined in the output, and the non-specified\n",
      " |      columns are dropped. (default of ``'drop'``).\n",
      " |      By specifying ``remainder='passthrough'``, all remaining columns that\n",
      " |      were not specified in `transformers` will be automatically passed\n",
      " |      through. This subset of columns is concatenated with the output of\n",
      " |      the transformers.\n",
      " |      By setting ``remainder`` to be an estimator, the remaining\n",
      " |      non-specified columns will use the ``remainder`` estimator. The\n",
      " |      estimator must support :term:`fit` and :term:`transform`.\n",
      " |      Note that using this feature requires that the DataFrame columns\n",
      " |      input at :term:`fit` and :term:`transform` have identical order.\n",
      " |  \n",
      " |  sparse_threshold : float, default = 0.3\n",
      " |      If the output of the different transformers contains sparse matrices,\n",
      " |      these will be stacked as a sparse matrix if the overall density is\n",
      " |      lower than this value. Use ``sparse_threshold=0`` to always return\n",
      " |      dense.  When the transformed output consists of all dense data, the\n",
      " |      stacked result will be dense, and this keyword will be ignored.\n",
      " |  \n",
      " |  n_jobs : int or None, optional (default=None)\n",
      " |      Number of jobs to run in parallel.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  transformer_weights : dict, optional\n",
      " |      Multiplicative weights for features per transformer. The output of the\n",
      " |      transformer is multiplied by these weights. Keys are transformer names,\n",
      " |      values the weights.\n",
      " |  \n",
      " |  verbose : boolean, optional(default=False)\n",
      " |      If True, the time elapsed while fitting each transformer will be\n",
      " |      printed as it is completed.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  transformers_ : list\n",
      " |      The collection of fitted transformers as tuples of\n",
      " |      (name, fitted_transformer, column). `fitted_transformer` can be an\n",
      " |      estimator, 'drop', or 'passthrough'. In case there were no columns\n",
      " |      selected, this will be the unfitted transformer.\n",
      " |      If there are remaining columns, the final element is a tuple of the\n",
      " |      form:\n",
      " |      ('remainder', transformer, remaining_columns) corresponding to the\n",
      " |      ``remainder`` parameter. If there are remaining columns, then\n",
      " |      ``len(transformers_)==len(transformers)+1``, otherwise\n",
      " |      ``len(transformers_)==len(transformers)``.\n",
      " |  \n",
      " |  named_transformers_ : Bunch object, a dictionary with attribute access\n",
      " |      Read-only attribute to access any transformer by given name.\n",
      " |      Keys are transformer names and values are the fitted transformer\n",
      " |      objects.\n",
      " |  \n",
      " |  sparse_output_ : boolean\n",
      " |      Boolean flag indicating wether the output of ``transform`` is a\n",
      " |      sparse matrix or a dense numpy array, which depends on the output\n",
      " |      of the individual transformers and the `sparse_threshold` keyword.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The order of the columns in the transformed feature matrix follows the\n",
      " |  order of how the columns are specified in the `transformers` list.\n",
      " |  Columns of the original feature matrix that are not specified are\n",
      " |  dropped from the resulting transformed feature matrix, unless specified\n",
      " |  in the `passthrough` keyword. Those columns specified with `passthrough`\n",
      " |  are added at the right to the output of the transformers.\n",
      " |  \n",
      " |  See also\n",
      " |  --------\n",
      " |  sklearn.compose.make_column_transformer : convenience function for\n",
      " |      combining the outputs of multiple transformer objects applied to\n",
      " |      column subsets of the original feature space.\n",
      " |  sklearn.compose.make_column_selector : convenience function for selecting\n",
      " |      columns based on datatype or the columns name with a regex pattern.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.compose import ColumnTransformer\n",
      " |  >>> from sklearn.preprocessing import Normalizer\n",
      " |  >>> ct = ColumnTransformer(\n",
      " |  ...     [(\"norm1\", Normalizer(norm='l1'), [0, 1]),\n",
      " |  ...      (\"norm2\", Normalizer(norm='l1'), slice(2, 4))])\n",
      " |  >>> X = np.array([[0., 1., 2., 2.],\n",
      " |  ...               [1., 1., 0., 1.]])\n",
      " |  >>> # Normalizer scales each row of X to unit norm. A separate scaling\n",
      " |  >>> # is applied for the two first and two last elements of each\n",
      " |  >>> # row independently.\n",
      " |  >>> ct.fit_transform(X)\n",
      " |  array([[0. , 1. , 0.5, 0.5],\n",
      " |         [0.5, 0.5, 0. , 1. ]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ColumnTransformer\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.utils.metaestimators._BaseComposition\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, transformers, remainder='drop', sparse_threshold=0.3, n_jobs=None, transformer_weights=None, verbose=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit all transformers using X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or DataFrame of shape [n_samples, n_features]\n",
      " |          Input data, of which specified subsets are used to fit the\n",
      " |          transformers.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples, ...), optional\n",
      " |          Targets for supervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : ColumnTransformer\n",
      " |          This estimator\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit all transformers, transform the data and concatenate results.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or DataFrame of shape [n_samples, n_features]\n",
      " |          Input data, of which specified subsets are used to fit the\n",
      " |          transformers.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples, ...), optional\n",
      " |          Targets for supervised learning.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)\n",
      " |          hstack of results of transformers. sum_n_components is the\n",
      " |          sum of n_components (output dimension) over transformers. If\n",
      " |          any result is a sparse matrix, everything will be converted to\n",
      " |          sparse matrices.\n",
      " |  \n",
      " |  get_feature_names(self)\n",
      " |      Get feature names from all transformers.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_names : list of strings\n",
      " |          Names of the features produced by transform.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      Valid parameter keys can be listed with ``get_params()``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X separately by each transformer, concatenate results.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or DataFrame of shape [n_samples, n_features]\n",
      " |          The data to be transformed by subset.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)\n",
      " |          hstack of results of transformers. sum_n_components is the\n",
      " |          sum of n_components (output dimension) over transformers. If\n",
      " |          any result is a sparse matrix, everything will be converted to\n",
      " |          sparse matrices.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  named_transformers_\n",
      " |      Access the fitted transformer by name.\n",
      " |      \n",
      " |      Read-only attribute to access any transformer by given name.\n",
      " |      Keys are transformer names and values are the fitted transformer\n",
      " |      objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338131b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
